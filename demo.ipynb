{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üîç RAG Demo with LangChain, Ollama, and pgvector (Dockerized)\n",
    "\n",
    "This notebook demonstrates a minimal Retrieval-Augmented Generation (RAG) pipeline using:\n",
    "\n",
    "- üß† **Ollama** (via `azaddjan/ollama:latest`) to run LLMs and generate embeddings locally\n",
    "- üóÉÔ∏è **pgvector** (via `azaddjan/pgvector:latest`) to store and query document embeddings\n",
    "- üîó **LangChain** to orchestrate embedding, retrieval, and generation\n",
    "\n",
    "We'll embed a few documents, store them in pgvector, retrieve similar ones for a query, and generate a final answer using `llama3`. Everything runs in Docker for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ñ∂Ô∏è Docker Compose Commands\n",
    "\n",
    "```bash\n",
    "# Start the services (Ollama on port 11500, Postgres on 5432)\n",
    "docker-compose up -d\n",
    "\n",
    "# Stop everything and remove volumes\n",
    "docker-compose down -v"
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# üõ† Install required packages\n",
    "!pip install --upgrade pip\n",
    "!pip install -r requirements.txt"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T14:48:03.921339Z",
     "start_time": "2025-07-22T14:48:03.914037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# Set dummy API key required for ChatOpenAI\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"ollama\"\n",
    "\n",
    "# Database credentials\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASSWORD = \"postgres\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "DB_NAME = \"test_rag\"\n",
    "\n",
    "postgres_connection_string = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\""
   ],
   "id": "5deda09be9de6cdf",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T14:52:53.133442Z",
     "start_time": "2025-07-22T14:52:53.103976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_test_database():\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"postgres\",\n",
    "        user=DB_USER,\n",
    "        password=DB_PASSWORD,\n",
    "        host=DB_HOST,\n",
    "        port=DB_PORT\n",
    "    )\n",
    "    conn.autocommit = True\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"SELECT 1 FROM pg_database WHERE datname = '{DB_NAME}';\")\n",
    "    exists = cur.fetchone()\n",
    "    if not exists:\n",
    "        cur.execute(f\"CREATE DATABASE {DB_NAME};\")\n",
    "        print(f\"‚úÖ Database '{DB_NAME}' created.\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è Database '{DB_NAME}' already exists.\")\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "create_test_database()"
   ],
   "id": "75540b0b55706947",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è Database 'test_rag' already exists.\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T14:53:00.804377Z",
     "start_time": "2025-07-22T14:53:00.775705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with psycopg2.connect(postgres_connection_string) as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"DROP TABLE IF EXISTS langchain_pg_embedding;\")\n",
    "        conn.commit()"
   ],
   "id": "83b98973d2b93df7",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T14:53:15.854890Z",
     "start_time": "2025-07-22T14:53:15.145544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_model = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text:v1.5\",\n",
    "    base_url=\"http://localhost:11500\"\n",
    ")\n",
    "\n",
    "texts = [\n",
    "    \"LangChain makes building LLM apps easier and modular.\",\n",
    "    \"Ollama runs open-source language models locally using containers.\",\n",
    "]\n",
    "\n",
    "metadatas = [{\"source\": \"doc1\"}, {\"source\": \"doc2\"}]\n",
    "\n",
    "vectorstore = PGVector.from_texts(\n",
    "    texts=texts,\n",
    "    embedding=embedding_model,\n",
    "    metadatas=metadatas,\n",
    "    collection_name=\"rag_demo\",\n",
    "    connection_string=postgres_connection_string,\n",
    "    use_jsonb=True\n",
    ")"
   ],
   "id": "b951087fd9f7c32d",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T14:54:30.683568Z",
     "start_time": "2025-07-22T14:53:47.325097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"How can I use containers to run AI models locally?\"\n",
    "retrieved_docs = vectorstore.similarity_search(query)\n",
    "context = \"\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Use the following context to answer the question:\\n\\n{context}\\n\\nQ: {question}\\nA:\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"llama3:8b\",\n",
    "    openai_api_base=\"http://localhost:11500/v1\",\n",
    "    openai_api_key=\"ollama\"\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"context\": context,\n",
    "    \"question\": query,\n",
    "})\n",
    "\n",
    "print(\"üîç RAG Answer:\\n\", response.content)"
   ],
   "id": "f2969e68e9973943",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç RAG Answer:\n",
      " You can use containers to run AI models, such as Ollama's open-source language models, locally by leveraging containerization technology. This allows you to isolate the model and its dependencies in a self-contained environment, making it easier to manage and deploy.\n",
      "\n",
      "Here are some steps to get started:\n",
      "\n",
      "1. Choose your preferred container runtime, such as Docker, and install it on your local machine.\n",
      "2. Grab the open-source language models from Ollama or another source, and convert them into a compatible format (e.g., PyTorch or TensorFlow).\n",
      "3. Create a new container using your chosen runtime, specifying the necessary dependencies and environment variables for the model to run correctly.\n",
      "4. Deploy the model inside the container by executing the relevant commands (e.g., `python train.py` or `python run.py`).\n",
      "5. Use container-specific tools (e.g., port mappings or volume mounts) to connect to the running container, allowing you to interact with the model through APIs, shell scripts, or your own applications.\n",
      "\n",
      "By using containers, you can ensure a consistent and reproducible environment for your AI models, making it easier to manage and maintain them.\n"
     ]
    }
   ],
   "execution_count": 55
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
