# ðŸ§  LocalAIEng: Local RAG with LangChain, Ollama, and pgvector

This project demonstrates a fully containerized Retrieval-Augmented Generation (RAG) pipeline using:

- **LangChain** for orchestration
- **Ollama** for local LLM + embedding models (via OpenAI-compatible API)
- **pgvector** for storing and querying vectorized documents
- **Docker Compose** for clean, reproducible setup